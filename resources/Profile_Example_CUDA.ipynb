{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoV0YWpWeX0Y",
    "outputId": "709250cb-0d42-4bbc-9708-b8aefef07c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVVwNqwjeYL0",
    "outputId": "da02e775-bfe9-427c-d9f7-82674e8feae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reduce.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce.cu\n",
    "/* This program will performe a reduce vectorA (size N)\n",
    "* with the + operation.\n",
    "+---------+ \n",
    "|111111111| \n",
    "+---------+\n",
    "     |\n",
    "     N\n",
    "\n",
    "vectorA   = all Ones\n",
    "N = Sum of vectorA\n",
    "*/\n",
    "#include <iostream>\n",
    "#include <sstream>\n",
    "#include <stdlib.h>\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "\n",
    "// CUDA macro wrapper for checking errors\n",
    "#define gpuErrCheck(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
    "inline void gpuAssert(cudaError_t code, const char* file, int line, bool abort = true)\n",
    "{\n",
    "    if (code != cudaSuccess)\n",
    "    {\n",
    "        std::cout << \"GPUassert: \" << cudaGetErrorString(code) << \" \" << file << \" \" << line << std::endl;\n",
    "        if (abort)\n",
    "        {\n",
    "            exit(code);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "// CPU reduce\n",
    "void reduce(int* vectorA, int* sum, int size)\n",
    "{\n",
    "    sum[0] = 0;\n",
    "    for (int i = 0; i < size; i++)\n",
    "        sum[0] += vectorA[i];\n",
    "}\n",
    "\n",
    "\n",
    "// EXERCISE\n",
    "// Read: https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/\n",
    "// Implement the reduce kernel based on the information\n",
    "// of the Nvidia blog post.\n",
    "// Implement both options, using no shared mem at all but global atomics\n",
    "// and using shared mem for the seconds recution phase.\n",
    "__global__ void cudaEvenFasterReduceAddition() {\n",
    "    //ToDo\n",
    "}\n",
    "\n",
    "\n",
    "// Already optimized reduce kernel using shared memory.\n",
    "__global__ void cudaReduceAddition(int* vectorA, int* sum)\n",
    "{\n",
    "    int globalIdx = 2 * blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    extern __shared__ int shmArray[];\n",
    "\n",
    "    shmArray[threadIdx.x] = vectorA[globalIdx];\n",
    "    shmArray[threadIdx.x + blockDim.x] = vectorA[globalIdx + blockDim.x];\n",
    "\n",
    "    for (int stride = blockDim.x; stride; stride >>= 1) {\n",
    "        if (threadIdx.x < stride) {\n",
    "            shmArray[threadIdx.x] += shmArray[threadIdx.x + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        sum[blockIdx.x] = shmArray[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "// Compare result vectors\n",
    "int compareResultVec(int* vectorCPU, int* vectorGPU, int size)\n",
    "{\n",
    "    int error = 0;\n",
    "    for (int i = 0; i < size; i++)\n",
    "    {\n",
    "        error += abs(vectorCPU[i] - vectorGPU[i]);\n",
    "    }\n",
    "    if (error == 0)\n",
    "    {\n",
    "        cout << \"No errors. All good!\" << endl;\n",
    "        return 0;\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        cout << \"Accumulated error: \" << error << endl;\n",
    "        return -1;\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Define the size of the vector: 1048576 elements\n",
    "    const int N = 1 << 20;\n",
    "    const int NBR_BLOCK = 512;\n",
    "\n",
    "    // Allocate and prepare input\n",
    "    int* hostVectorA = new int[N];\n",
    "    int hostSumCPU[1];\n",
    "    int hostSumGPU[1];\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        hostVectorA[i] = 1;\n",
    "    }\n",
    "\n",
    "    // Alloc N times size of int at address of deviceVector[A-C]\n",
    "    int* deviceVectorA;\n",
    "    int* deviceSum;\n",
    "    gpuErrCheck(cudaMalloc(&deviceVectorA, N * sizeof(int)));\n",
    "    gpuErrCheck(cudaMalloc(&deviceSum, NBR_BLOCK* sizeof(int)));\n",
    "\n",
    "    // Copy data from host to device\n",
    "    gpuErrCheck(cudaMemcpy(deviceVectorA, hostVectorA, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "\n",
    "    // Run the vector kernel on the CPU\n",
    "    reduce(hostVectorA, hostSumCPU, N);\n",
    "\n",
    "    // Run kernel on all elements on the GPU\n",
    "    cudaReduceAddition <<<NBR_BLOCK, 1024, 2 * 1024 * sizeof(int)>>> (deviceVectorA, deviceSum);\n",
    "    gpuErrCheck(cudaPeekAtLastError());\n",
    "    cudaReduceAddition <<<1, NBR_BLOCK / 2, NBR_BLOCK * sizeof(int) >> > (deviceSum, deviceSum);\n",
    "    gpuErrCheck(cudaPeekAtLastError());\n",
    "\n",
    "    // Copy the result stored in device_y back to host_y\n",
    "    gpuErrCheck(cudaMemcpy(hostSumGPU, deviceSum, sizeof(int), cudaMemcpyDeviceToHost));\n",
    "\n",
    "    // Check for errors\n",
    "    auto isValid = compareResultVec(hostSumCPU, hostSumGPU, 1);\n",
    "\n",
    "    // Free memory on device\n",
    "    gpuErrCheck(cudaFree(deviceVectorA));\n",
    "    gpuErrCheck(cudaFree(deviceSum));\n",
    "\n",
    "    // Free memory on host\n",
    "    delete[] hostVectorA;\n",
    "\n",
    "    return isValid;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXRAf_-9lub9"
   },
   "source": [
    "**Attention:** If you get a K80, you should compile it like this:\n",
    "`!nvcc -arch=sm_37 -o reduce reduce.cu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4lLHNUzHef9q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvcc\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o reduce reduce.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57IzTD97eqDX",
    "outputId": "6aceadaf-3698-4c1a-e9d9-0d69d0c4535c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: ./reduce\n"
     ]
    }
   ],
   "source": [
    "!./reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p-YpNLPnMEV"
   },
   "source": [
    "**Profiling on old GPU (K80)**\n",
    "\n",
    "*   `!nvprof --print-gpu-trace ./reduce`\n",
    "*   `!nvprof --analysis-metrics -o reduce_out.nvprof ./reduce`\n",
    "*   --> Use the Visual Profiler\n",
    "\n",
    "**Profiling on newer GPUs**\n",
    "\n",
    "*   `!nsys profile -f true -o reduce_out -t cuda ./reduce`\n",
    "*   `!nsys stats --report gputrace reduce.qdrep`\n",
    "*   `!nsys stats reduce.qdrep`\n",
    "*   `!ncu -f -o reduce --set full ./reduce`\n",
    "*   --> Nvidia Nsight Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPbbe5AaUU4Y"
   },
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FszJbI5nVjz",
    "outputId": "7e836e7a-2ad7-41ab-99a2-906a6fec252f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvprof\n"
     ]
    }
   ],
   "source": [
    "!nvprof --print-gpu-trace ./reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZrPgfUgn1DX",
    "outputId": "5459d5d2-fdad-4f19-b5c5-49f516d13d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvprof\n"
     ]
    }
   ],
   "source": [
    "!nvprof --analysis-metrics -o reduce_out.nvprof ./reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTzsnXS4UYtc"
   },
   "source": [
    "Use the file *reduce_out.nvprof* with the visual profiler of Nvidia, which you have locally installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lrwzKmITUmJp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nsys\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -f true --stats=true -o reduce_out -t cuda ./reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nsys\n"
     ]
    }
   ],
   "source": [
    "!nsys stats reduce_out.qdrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "fhnw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
