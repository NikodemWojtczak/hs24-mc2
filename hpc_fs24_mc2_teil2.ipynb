{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hTpYz3aFpg0"
   },
   "source": [
    "HPC Mini-Challenge 2 - Acceleration in Data Science\n",
    " Part 2: GPU\n",
    " FHNW - FS2024\n",
    "\n",
    "Original by S. Suter, adapted by S. Marcin and M. Stutz\n",
    "\n",
    "Submitted by: Nikodem Wojtczak\n",
    "\n",
    " Resources\n",
    "* [Overview of GPU Programming](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
    "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
    "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
    "* Lectures and examples from the PAC (parallel computing) computer science course, see folder \"resources\"\n",
    "* CSCS \"High-Performance Computing with Python\" course, Day 3:\n",
    "    - JIT Numba GPU 1 + 2\n",
    "    - https://youtu.be/E4REVbCVxNQ\n",
    "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
    "    - See also the current tutorial from 2021\n",
    "* [Google CoLab](https://colab.research.google.com/) or your own GPU if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1749661142840,
     "user": {
      "displayName": "Nikodem Wojtczak",
      "userId": "05898537834362297500"
     },
     "user_tz": -120
    },
    "id": "BvFYnwKY6q1y"
   },
   "outputs": [],
   "source": [
    "from numba import config\n",
    "\n",
    "config.CUDA_ENABLE_PYNVJITLINK = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1672,
     "status": "ok",
     "timestamp": 1749661148320,
     "user": {
      "displayName": "Nikodem Wojtczak",
      "userId": "05898537834362297500"
     },
     "user_tz": -120
    },
    "id": "ag4S15AdIuBz",
    "outputId": "75333621-a1b6-46f4-ad7c-1c63de8fedc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Environment Diagnostics ---\n",
      "Python Version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "\n",
      "Numba CUDA Diagnostics:\n",
      "Found 1 CUDA devices\n",
      "id 0             b'Tesla T4'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 4\n",
      "                              PCI Bus ID: 0\n",
      "                                    UUID: GPU-7761e32a-6ec9-7b68-c1e0-b09b0b4acf82\n",
      "                                Watchdog: Disabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "\n",
      "CUDA_HOME environment variable: Not Set\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from numba import cuda\n",
    "import os\n",
    "\n",
    "print(\"--- Environment Diagnostics ---\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nNumba CUDA Diagnostics:\")\n",
    "    if cuda.is_available():\n",
    "        cuda.detect()\n",
    "    else:\n",
    "        print(\"Numba could not find a CUDA-enabled GPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Numba CUDA detection: {e}\")\n",
    "\n",
    "# Check for CUDA_HOME environment variable\n",
    "print(f\"\\nCUDA_HOME environment variable: {os.environ.get('CUDA_HOME', 'Not Set')}\")\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 795,
     "status": "ok",
     "timestamp": 1749661153156,
     "user": {
      "displayName": "Nikodem Wojtczak",
      "userId": "05898537834362297500"
     },
     "user_tz": -120
    },
    "id": "hXgaKjmoFpg3",
    "outputId": "cf774eb2-aea7-4c77-fa44-4a45a7b85859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba CUDA vectorize test successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Dummy Beispiel zum testen mit Numba\n",
    "import math\n",
    "from numba import vectorize, cuda\n",
    "import numpy as np\n",
    "\n",
    "# This check is important to see if a GPU is available.\n",
    "if cuda.is_available():\n",
    "\n",
    "    @vectorize([\"float32(float32)\"], target=\"cuda\")\n",
    "    def gpu_sqrt(x):\n",
    "        return math.sqrt(x)\n",
    "\n",
    "    a = np.arange(4096, dtype=np.float32)\n",
    "    # Move data to GPU, compute, and bring back\n",
    "    a_device = cuda.to_device(a)\n",
    "    result_device = gpu_sqrt(a_device)\n",
    "    result_host = result_device.copy_to_host()\n",
    "    print(\"Numba CUDA vectorize test successful.\")\n",
    "else:\n",
    "    print(\"Numba CUDA not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfBE1O-MFpg3"
   },
   "source": [
    "## 5 GPU Reconstruction\n",
    "\n",
    "Implement an SVD reconstruction variant on the GPU or in a hybrid setting. You may use code from the first part. Carefully choose which parts of the algorithm should be implemented in a GPU kernel and which are more efficiently executed on the CPU. Incorporate insights from the first part. At least one component of the algorithm must be implemented in a GPU kernel. Document any assumptions you make for simplification. Evaluate whether you want to use CuPy or Numba.\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplication](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import uuid\n",
    "import imageio.v3 as imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "source_subfolder = \"VeryMildDemented\"\n",
    "processed_subfolder = \"VeryMildDemented_png\"\n",
    "base_dir = \"adni_png\"\n",
    "source_folders = os.path.join(base_dir, source_subfolder)\n",
    "processed_folders = os.path.join(base_dir, processed_subfolder)\n",
    "standard_size = (256, 256)\n",
    "\n",
    "if not os.path.exists(processed_folders):\n",
    "    os.makedirs(processed_folders)\n",
    "\n",
    "jpg_files = sorted(glob.glob(f\"{source_folders}/*.jpg\"))\n",
    "if jpg_files:\n",
    "    for jpg_file in jpg_files:\n",
    "        png_filename = os.path.splitext(os.path.basename(jpg_file))[0] + \".png\"\n",
    "        png_filepath = os.path.join(processed_folders, png_filename)\n",
    "        if not os.path.exists(png_filepath):\n",
    "            with Image.open(jpg_file) as img_pil:\n",
    "                img_processed = img_pil.convert(\"L\").resize(\n",
    "                    standard_size, Image.Resampling.LANCZOS\n",
    "                )\n",
    "                img_processed.save(png_filepath)\n",
    "\n",
    "png_files = sorted(glob.glob(f\"{processed_folders}/*.png\"))\n",
    "im_cpu, u_cpu, s_cpu, vt_cpu = None, None, None, None\n",
    "if png_files:\n",
    "    im_cpu = np.array(Image.open(png_files[0])).astype(np.float32)\n",
    "    if im_cpu.max() > 0:\n",
    "        im_cpu = (im_cpu - im_cpu.min()) / (im_cpu.max() - im_cpu.min())\n",
    "    u_cpu, s_cpu, vt_cpu = np.linalg.svd(im_cpu, full_matrices=False)\n",
    "else:\n",
    "    print(\"No processed images found to run GPU reconstruction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64941,
     "status": "ok",
     "timestamp": 1749661226203,
     "user": {
      "displayName": "Nikodem Wojtczak",
      "userId": "05898537834362297500"
     },
     "user_tz": -120
    },
    "id": "_lftiQkUFpg3",
    "outputId": "c50e0039-3b79-4571-f273-19a553682ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GPU Reconstruction Performance (k=100) ---\n",
      "CuPy execution time: 1427.9841 ms\n",
      "Numba Kernel execution time: 249.4730 ms\n",
      "CPU (NumPy Broadcast) time: 0.6553 ms\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "\n",
    "# --- Option 1: CuPy Implementation (Leverages optimized cuBLAS libraries) ---\n",
    "def reconstruct_svd_cupy(u_cpu, s_cpu, vt_cpu, k):\n",
    "    \"\"\"SVD reconstruction using CuPy, a drop-in replacement for NumPy.\"\"\"\n",
    "    # 1. Move data from CPU (host) to GPU (device)\n",
    "    u_gpu = cp.asarray(u_cpu)\n",
    "    s_gpu = cp.asarray(s_cpu)\n",
    "    vt_gpu = cp.asarray(vt_cpu)\n",
    "\n",
    "    # 2. Perform the computation on the GPU using CuPy's NumPy-like syntax\n",
    "    reco_gpu = (u_gpu[:, :k] * s_gpu[:k]) @ vt_gpu[:k, :]\n",
    "\n",
    "    # 3. Move the result from GPU back to CPU\n",
    "    reco_cpu = cp.asnumpy(reco_gpu)\n",
    "    return reco_cpu\n",
    "\n",
    "\n",
    "# --- Option 2: Numba Custom Kernel (More manual control) ---\n",
    "@cuda.jit\n",
    "def reconstruct_svd_numba_kernel(reco, u, s, vt, k):\n",
    "    \"\"\"Custom CUDA kernel for SVD reconstruction.\"\"\"\n",
    "    i, j = cuda.grid(2)\n",
    "    m, n = reco.shape\n",
    "    if i < m and j < n:\n",
    "        temp_sum = 0.0\n",
    "        for l in range(k):\n",
    "            temp_sum += u[i, l] * s[l] * vt[l, j]\n",
    "        reco[i, j] = temp_sum\n",
    "\n",
    "\n",
    "def reconstruct_svd_numba_gpu(u_cpu, s_cpu, vt_cpu, k):\n",
    "    \"\"\"Wrapper function to run the Numba CUDA kernel.\"\"\"\n",
    "    m, n = u_cpu.shape[0], vt_cpu.shape[1]\n",
    "    u_gpu = cuda.to_device(u_cpu)\n",
    "    s_gpu = cuda.to_device(s_cpu)\n",
    "    vt_gpu = cuda.to_device(vt_cpu)\n",
    "    reco_gpu = cuda.device_array((m, n), dtype=np.float32)\n",
    "\n",
    "    threads_per_block = (16, 16)\n",
    "    blocks_per_grid_x = math.ceil(m / threads_per_block[0])\n",
    "    blocks_per_grid_y = math.ceil(n / threads_per_block[1])\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "    reconstruct_svd_numba_kernel[blocks_per_grid, threads_per_block](\n",
    "        reco_gpu, u_gpu, s_gpu, vt_gpu, k\n",
    "    )\n",
    "    return reco_gpu.copy_to_host()\n",
    "\n",
    "\n",
    "# --- Comparison ---\n",
    "if cuda.is_available() and u_cpu is not None:\n",
    "    k = 100\n",
    "    print(f\"--- GPU Reconstruction Performance (k={k}) ---\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    reco_cupy = reconstruct_svd_cupy(u_cpu, s_cpu, vt_cpu, k)\n",
    "    cupy_time = (time.perf_counter() - start) * 1000\n",
    "    print(f\"CuPy execution time: {cupy_time:.4f} ms\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    reco_numba = reconstruct_svd_numba_gpu(u_cpu, s_cpu, vt_cpu, k)\n",
    "    numba_time = (time.perf_counter() - start) * 1000\n",
    "    print(f\"Numba Kernel execution time: {numba_time:.4f} ms\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    reco_cpu_broadcast = (u_cpu[:, :k] * s_cpu[:k]) @ vt_cpu[:k, :]\n",
    "    cpu_time = (time.perf_counter() - start) * 1000\n",
    "    print(f\"CPU (NumPy Broadcast) time: {cpu_time:.4f} ms\")\n",
    "else:\n",
    "    print(\"GPU not available or no images found, skipping GPU execution.\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lERkaziFpg4"
   },
   "source": [
    " **Analysis of Initial Results:**\n",
    "\n",
    " The initial test on a single 256x256 image clearly demonstrates a critical principle of accelerated computing: **overhead is dominant for small tasks**. The results were:\n",
    " - **CPU (NumPy): 0.66 ms**\n",
    " - **GPU (Numba Kernel): 249.47 ms**\n",
    " - **GPU (CuPy): 1427.98 ms**\n",
    "\n",
    " The CPU is orders of magnitude faster. This is because the time required to allocate GPU memory, transfer the small matrices from CPU to GPU, launch the kernel, and copy the result back completely dwarfs the actual computation time. The NumPy operation on the CPU has virtually zero overhead and wins easily.\n",
    "\n",
    " **CuPy vs. Numba:** Surprisingly, the custom Numba kernel was significantly faster than CuPy. This is unusual, as CuPy typically calls highly optimized libraries. This result likely points to a high startup or first-call overhead for CuPy in the Colab environment for this specific problem size, whereas the Numba JIT compilation was more efficient in this isolated case. For larger, more complex tasks, CuPy is still the recommended approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN28bGx_Fpg4"
   },
   "source": [
    "## 5.2 GPU Kernel Performance\n",
    "\n",
    "### 5.2.1 Blocks and Input Size\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplication](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
    "* [NVIDIA Chapter on \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
    "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
    "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
    "\n",
    "Conduct 2-3 experiments with different block configurations and input data sizes. For this, create a new dataset with arbitrarily large matrices, since the GPU is particularly well-suited for processing large inputs (use these differently sized matrices for all subsequent comparisons and tasks as well). Measure the performance of the GPU kernel using appropriate functions. Which block size, depending on the input size, proved to be the most successful in your experiments? What do you think are the reasons for this? What are the performance differences between your CPU and GPU implementation? Discuss your analysis (optionally with graphics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1045,
     "status": "ok",
     "timestamp": 1749661227249,
     "user": {
      "displayName": "Nikodem Wojtczak",
      "userId": "05898537834362297500"
     },
     "user_tz": -120
    },
    "id": "Fg6OUcr_Fpg4",
    "outputId": "371d3cb6-c6fc-4c12-b75d-ad146b403258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPU Kernel Benchmark: Block Size vs. Input Size ---\n",
      "\n",
      "Matrix Size: 512x512\n",
      "Block (8, 8)  : Total GPU: 6.4735 ms (Transfer In: 2.3904 ms, Kernel: 3.5957 ms, Transfer Out: 0.4875 ms)\n",
      "Block (16, 16): Total GPU: 5.8952 ms (Transfer In: 1.8877 ms, Kernel: 3.5481 ms, Transfer Out: 0.4594 ms)\n",
      "Block (32, 32): Total GPU: 9.4175 ms (Transfer In: 2.1044 ms, Kernel: 6.8998 ms, Transfer Out: 0.4134 ms)\n",
      "CPU (NumPy): 2.4269 ms\n",
      "\n",
      "Matrix Size: 1024x1024\n",
      "Block (8, 8)  : Total GPU: 18.6582 ms (Transfer In: 4.1288 ms, Kernel: 13.4996 ms, Transfer Out: 1.0298 ms)\n",
      "Block (16, 16): Total GPU: 18.3348 ms (Transfer In: 3.7705 ms, Kernel: 13.5362 ms, Transfer Out: 1.0281 ms)\n",
      "Block (32, 32): Total GPU: 29.3567 ms (Transfer In: 3.1569 ms, Kernel: 25.2059 ms, Transfer Out: 0.9940 ms)\n",
      "CPU (NumPy): 7.7386 ms\n",
      "\n",
      "Matrix Size: 2048x2048\n",
      "Block (8, 8)  : Total GPU: 69.1929 ms (Transfer In: 9.3672 ms, Kernel: 52.9415 ms, Transfer Out: 6.8842 ms)\n",
      "Block (16, 16): Total GPU: 69.2668 ms (Transfer In: 9.2173 ms, Kernel: 53.2140 ms, Transfer Out: 6.8356 ms)\n",
      "Block (32, 32): Total GPU: 116.0541 ms (Transfer In: 10.0764 ms, Kernel: 98.8409 ms, Transfer Out: 7.1368 ms)\n",
      "CPU (NumPy): 37.5936 ms\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "def benchmark_gpu_kernel(matrix_size, block_size_2d, k):\n",
    "    \"\"\"Benchmarks the Numba kernel for a given matrix and block size.\"\"\"\n",
    "    u_cpu = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "    s_cpu = np.random.rand(matrix_size).astype(np.float32)\n",
    "    vt_cpu = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "\n",
    "    start_transfer = time.perf_counter()\n",
    "    u_gpu = cuda.to_device(u_cpu)\n",
    "    s_gpu = cuda.to_device(s_cpu)\n",
    "    vt_gpu = cuda.to_device(vt_cpu)\n",
    "    reco_gpu = cuda.device_array((matrix_size, matrix_size), dtype=np.float32)\n",
    "    cuda.synchronize()\n",
    "    transfer_time_ms = (time.perf_counter() - start_transfer) * 1000\n",
    "\n",
    "    threads_per_block = block_size_2d\n",
    "    blocks_per_grid_x = math.ceil(matrix_size / threads_per_block[0])\n",
    "    blocks_per_grid_y = math.ceil(matrix_size / threads_per_block[1])\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "    start_kernel = time.perf_counter()\n",
    "    reconstruct_svd_numba_kernel[blocks_per_grid, threads_per_block](\n",
    "        reco_gpu, u_gpu, s_gpu, vt_gpu, k\n",
    "    )\n",
    "    cuda.synchronize()\n",
    "    kernel_time_ms = (time.perf_counter() - start_kernel) * 1000\n",
    "\n",
    "    start_transfer_back = time.perf_counter()\n",
    "    reco_cpu = reco_gpu.copy_to_host()\n",
    "    cuda.synchronize()\n",
    "    transfer_back_time_ms = (time.perf_counter() - start_transfer_back) * 1000\n",
    "\n",
    "    total_gpu_time_ms = transfer_time_ms + kernel_time_ms + transfer_back_time_ms\n",
    "\n",
    "    return total_gpu_time_ms, transfer_time_ms, kernel_time_ms, transfer_back_time_ms\n",
    "\n",
    "\n",
    "if cuda.is_available():\n",
    "    matrix_sizes = [512, 1024, 2048]\n",
    "    block_configs = [(8, 8), (16, 16), (32, 32)]\n",
    "    k_bench = 256\n",
    "    results = {}\n",
    "\n",
    "    print(\"\\n--- GPU Kernel Benchmark: Block Size vs. Input Size ---\")\n",
    "    for size in matrix_sizes:\n",
    "        print(f\"\\nMatrix Size: {size}x{size}\")\n",
    "        results[size] = {}\n",
    "        for block in block_configs:\n",
    "            if block[0] * block[1] > cuda.get_current_device().MAX_THREADS_PER_BLOCK:\n",
    "                print(f\"Block size {block} too large for device, skipping.\")\n",
    "                continue\n",
    "            total_time, transfer_in, kernel_time, transfer_out = benchmark_gpu_kernel(\n",
    "                size, block, k_bench\n",
    "            )\n",
    "            results[size][block] = total_time\n",
    "            print(\n",
    "                f\"Block {str(block):<8}: Total GPU: {total_time:.4f} ms (Transfer In: {transfer_in:.4f} ms, Kernel: {kernel_time:.4f} ms, Transfer Out: {transfer_out:.4f} ms)\"\n",
    "            )\n",
    "\n",
    "        # Benchmark CPU for comparison\n",
    "        u_cpu_bench = np.random.rand(size, size).astype(np.float32)\n",
    "        s_cpu_bench = np.random.rand(size).astype(np.float32)\n",
    "        vt_cpu_bench = np.random.rand(size, size).astype(np.float32)\n",
    "        start_cpu = time.perf_counter()\n",
    "        _ = (u_cpu_bench[:, :k_bench] * s_cpu_bench[:k_bench]) @ vt_cpu_bench[\n",
    "            :k_bench, :\n",
    "        ]\n",
    "        time_cpu_ms = (time.perf_counter() - start_cpu) * 1000\n",
    "        results[size][\"cpu\"] = time_cpu_ms\n",
    "        print(f\"CPU (NumPy): {time_cpu_ms:.4f} ms\")\n",
    "else:\n",
    "    print(\"GPU not available, skipping benchmark.\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYuG1i4FFpg4"
   },
   "source": [
    " **Analysis of Results:**\n",
    "\n",
    " **CPU vs. GPU Performance:** The results clearly show that for this problem, the **CPU is faster overall, even at large matrix sizes**. For the 2048x2048 matrix, the total GPU time was **69.19 ms** while the CPU took only **37.59 ms**. Although the GPU kernel's computation time (52.94 ms) was approaching the CPU's time, the data transfer overhead (~16 ms) erased any potential gains. This demonstrates that this problem is **I/O bound**; the bottleneck is not the computation itself, but the time spent moving data to and from the GPU.\n",
    "\n",
    " **Optimal Block Size:** Across all matrix sizes, the block size of **(16, 16)** consistently provided the best or near-best kernel performance. The largest block size, **(32, 32)**, was consistently the slowest. This is expected behavior: a (32, 32) block uses 1024 threads, the maximum for most GPUs. This can cause high \"register pressure,\" where the GPU's Streaming Multiprocessors (SMs) don't have enough local register memory for all active threads, forcing them to spill data to slower memory. A more moderate size like (16, 16) (256 threads) provides a better balance of parallelism and available resources, leading to higher efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7piTtOYFpg4"
   },
   "source": [
    "### 5.2.2 Shared Memory on the GPU\n",
    "Optimize your implementation above by using the shared memory of the GPU. Again, conduct several experiments with different data sizes and evaluate the speedup compared to the CPU implementation.\n",
    "\n",
    "Links:\n",
    "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
    "* [Examples: Matrix Multiplication and Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1749661366875,
     "user": {
      "displayName": "Nikodem Wojtczak",
      "userId": "05898537834362297500"
     },
     "user_tz": -120
    },
    "id": "rsJgKnB3Fpg5",
    "outputId": "7abb84c7-efb8-4f3f-8a65-2414d35bb4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Shared Memory Kernel Benchmark ---\n",
      "\n",
      "Matrix Size: 512x512\n",
      "Basic Kernel Time:    9.4326 ms\n",
      "Shared Memory Kernel: 188.7665 ms (Transfer In: 1.9962 ms, Kernel: 186.2322 ms, Transfer Out: 0.5380 ms)\n",
      "Speedup from Shared Memory: 0.05x\n",
      "\n",
      "Matrix Size: 1024x1024\n",
      "Basic Kernel Time:    57.4540 ms\n",
      "Shared Memory Kernel: 58.2963 ms (Transfer In: 3.5657 ms, Kernel: 53.6509 ms, Transfer Out: 1.0796 ms)\n",
      "Speedup from Shared Memory: 0.99x\n",
      "\n",
      "Matrix Size: 2048x2048\n",
      "Basic Kernel Time:    316.0632 ms\n",
      "Shared Memory Kernel: 174.6644 ms (Transfer In: 9.8910 ms, Kernel: 157.4988 ms, Transfer Out: 7.2746 ms)\n",
      "Speedup from Shared Memory: 1.81x\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "TILE_DIM = 16\n",
    "import numba\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def reconstruct_svd_shared_mem_kernel(reco, u, s, vt, k):\n",
    "    s_u = cuda.shared.array(shape=(TILE_DIM, TILE_DIM), dtype=numba.float32)\n",
    "    s_vt = cuda.shared.array(shape=(TILE_DIM, TILE_DIM), dtype=numba.float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "\n",
    "    tmp = 0.0\n",
    "\n",
    "    num_tiles = math.ceil(k / TILE_DIM)\n",
    "\n",
    "    for i in range(num_tiles):\n",
    "        tile_col_u = i * TILE_DIM + ty\n",
    "        tile_row_vt = i * TILE_DIM + tx\n",
    "\n",
    "        if x < u.shape[0] and tile_col_u < k:\n",
    "            s_u[tx, ty] = u[x, tile_col_u] * s[tile_col_u]\n",
    "        else:\n",
    "            s_u[tx, ty] = 0.0\n",
    "\n",
    "        if y < vt.shape[1] and tile_row_vt < k:\n",
    "            s_vt[tx, ty] = vt[tile_row_vt, y]\n",
    "        else:\n",
    "            s_vt[tx, ty] = 0.0\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for j in range(TILE_DIM):\n",
    "            tmp += s_u[tx, j] * s_vt[j, ty]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    if x < reco.shape[0] and y < reco.shape[1]:\n",
    "        reco[x, y] = tmp\n",
    "\n",
    "\n",
    "def benchmark_shared_mem_kernel(matrix_size, k):\n",
    "    u_cpu = np.random.rand(matrix_size, k).astype(np.float32)\n",
    "    s_cpu = np.random.rand(k).astype(np.float32)\n",
    "    vt_cpu = np.random.rand(k, matrix_size).astype(np.float32)\n",
    "\n",
    "    start_transfer = time.perf_counter()\n",
    "    u_gpu = cuda.to_device(u_cpu)\n",
    "    s_gpu = cuda.to_device(s_cpu)\n",
    "    vt_gpu = cuda.to_device(vt_cpu)\n",
    "    reco_gpu = cuda.device_array((matrix_size, matrix_size), dtype=np.float32)\n",
    "    cuda.synchronize()\n",
    "    transfer_time_ms = (time.perf_counter() - start_transfer) * 1000\n",
    "\n",
    "    threads_per_block = (TILE_DIM, TILE_DIM)\n",
    "    blocks_per_grid_x = math.ceil(matrix_size / threads_per_block[0])\n",
    "    blocks_per_grid_y = math.ceil(matrix_size / threads_per_block[1])\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "    start_kernel = time.perf_counter()\n",
    "    reconstruct_svd_shared_mem_kernel[blocks_per_grid, threads_per_block](\n",
    "        reco_gpu, u_gpu, s_gpu, vt_gpu, k\n",
    "    )\n",
    "    cuda.synchronize()\n",
    "    kernel_time_ms = (time.perf_counter() - start_kernel) * 1000\n",
    "\n",
    "    start_transfer_back = time.perf_counter()\n",
    "    cuda.synchronize()\n",
    "    transfer_back_time_ms = (time.perf_counter() - start_transfer_back) * 1000\n",
    "\n",
    "    total_gpu_time_ms = transfer_time_ms + kernel_time_ms + transfer_back_time_ms\n",
    "\n",
    "    return total_gpu_time_ms, transfer_time_ms, kernel_time_ms, transfer_back_time_ms\n",
    "\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(\"\\n--- Shared Memory Kernel Benchmark ---\")\n",
    "    k_shared = 256\n",
    "    matrix_sizes = [512, 1024, 2048]\n",
    "\n",
    "    for size in matrix_sizes:\n",
    "        print(f\"\\nMatrix Size: {size}x{size}\")\n",
    "\n",
    "        time_basic, _, _, _ = benchmark_gpu_kernel(size, (TILE_DIM, TILE_DIM), size)\n",
    "        print(f\"Basic Kernel Time:    {time_basic:.4f} ms\")\n",
    "\n",
    "        time_shared, transfer_in_shared, kernel_time_shared, transfer_out_shared = (\n",
    "            benchmark_shared_mem_kernel(size, size)\n",
    "        )\n",
    "        print(\n",
    "            f\"Shared Memory Kernel: {time_shared:.4f} ms (Transfer In: {transfer_in_shared:.4f} ms, Kernel: {kernel_time_shared:.4f} ms, Transfer Out: {transfer_out_shared:.4f} ms)\"\n",
    "        )\n",
    "\n",
    "        if time_shared > 0:\n",
    "            speedup = time_basic / time_shared\n",
    "            print(f\"Speedup from Shared Memory: {speedup:.2f}x\")\n",
    "else:\n",
    "    print(\"GPU not available, skipping shared memory benchmark.\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CPU-only Benchmark for Comparison ---\n",
      "\n",
      "Matrix Size: 512x512\n",
      "CPU (NumPy): 1.0668 ms\n",
      "\n",
      "Matrix Size: 1024x1024\n",
      "CPU (NumPy): 3.7195 ms\n",
      "\n",
      "Matrix Size: 2048x2048\n",
      "CPU (NumPy): 7.0828 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "matrix_sizes = [512, 1024, 2048]\n",
    "k_bench = 256  # k value used in GPU benchmarks\n",
    "\n",
    "for size in matrix_sizes:\n",
    "\n",
    "    u_cpu_bench = np.random.rand(size, size).astype(np.float32)\n",
    "    s_cpu_bench = np.random.rand(size).astype(np.float32)\n",
    "    vt_cpu_bench = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "    _ = (u_cpu_bench[:, :k_bench] * s_cpu_bench[:k_bench]) @ vt_cpu_bench[:k_bench, :]\n",
    "\n",
    "    start_cpu = time.perf_counter()\n",
    "    num_runs = 10\n",
    "    for _ in range(num_runs):\n",
    "        _ = (u_cpu_bench[:, :k_bench] * s_cpu_bench[:k_bench]) @ vt_cpu_bench[\n",
    "            :k_bench, :\n",
    "        ]\n",
    "\n",
    "    time_cpu_ms = ((time.perf_counter() - start_cpu) / num_runs) * 1000\n",
    "\n",
    "    print(f\"CPU (NumPy): {time_cpu_ms:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FESqZrUYFpg5"
   },
   "source": [
    "What are your findings regarding GPU memory allocation and data transfer to the GPU? Interpret your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adKqi7PiFpg5"
   },
   "source": [
    "**Analysis of Shared Memory Benchmark vs. CPU:**\n",
    "\n",
    "**1. CPU is Dramatically Faster:** The direct comparison confirms the trend from the previous benchmark. The CPU is overwhelmingly faster when considering the total end-to-end time. For the 2048x2048 case, the optimized GPU process took **174.66 ms**, whereas the CPU finished in just **7.08 ms**. This is a speedup of only **0.04x**, meaning the GPU was ~25 times slower.\n",
    "\n",
    "**2. Kernel Optimization Shows Promise:** Despite the slow overall time, the shared memory optimization *did* make the GPU kernel itself more efficient. The kernel time of **157.50 ms** for the shared memory version at 2048x2048 is a significant improvement over the basic kernel. This shows the optimization technique is valid.\n",
    "\n",
    "**3. I/O is the Unbeatable Bottleneck:** The results prove that this application is fundamentally limited by data transfer time. The ~17 ms spent on moving data to and from the GPU for the 2048x2048 case is more than double the entire execution time of the CPU. This demonstrates that for a problem to benefit from GPU acceleration, the computational savings must be large enough to overcome this fixed I/O cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsUw73MzFpg5"
   },
   "source": [
    " 5.2.3 Bonus: Further Optimizations\n",
    "Further optimize your implementation from above. To be successful, you need to increase data reuse even more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 85872,
     "status": "aborted",
     "timestamp": 1749660830364,
     "user": {
      "displayName": "Nikodem Wojtczak",
      "userId": "05898537834362297500"
     },
     "user_tz": -120
    },
    "id": "Ng6jgSC7Fpg5"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4CGUhrhFpg5"
   },
   "source": [
    " 5.3 NVIDIA Profiler\n",
    "\n",
    "Use an NVIDIA performance profiler to identify bottlenecks in your code or to compare different implementations (blocks, memory, etc.).\n",
    "\n",
    "* See the example: example_profiling_CUDA.ipynb\n",
    "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) for profiling code and inspecting results (latest version)\n",
    "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
    "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
    "\n",
    "> You can install NVIDIA Nsight Systems and the Nvidia Visual Profiler on your PC and visualize performance results from a remote instance, even if you do not have a GPU on your PC. To do this, you can generate the ``*.qdrep`` file and then load it locally.\n",
    "\n",
    "Document your analysis with 1-2 visualizations if possible, and describe which bottlenecks you found or were able to resolve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En_tjjgbFpg5"
   },
   "source": [
    "**Note  this is a file generates for profile_script.py** \n",
    "\n",
    "\n",
    "![Nsight Profiler Timeline](Nsight.png)\n",
    "\n",
    "The profiler timeline clearly reveals a system-level bottleneck where the GPU remains completely idle for some time. This idle time is caused by the CPU-bound setup phase of the script, where data is prepared before any computation can be offloaded to the GPU. While this high-level bottleneck remains, the implementation of the shared memory kernel was designed to resolve a more fine-grained bottleneck within the GPU computation by reducing slow global memory accesses, which was confirmed by our previous benchmark timings.\n",
    "\n",
    "**NVIDIA Profiler Analysis using profile_script.py**\n",
    "\n",
    "To analyze the GPU performance bottlenecks in detail, a dedicated profiling script (`profile_script.py`) was created. This script isolates the core GPU kernels to provide cleaner profiling results. The script contains:\n",
    "\n",
    "1. **Kernel Definitions**: Both the basic SVD reconstruction kernel and the shared memory optimized kernel from our previous implementations\n",
    "2. **Controlled Setup**: Creates a 1024x1024 matrix with random data directly on the GPU to minimize setup overhead\n",
    "3. **Sequential Execution**: Runs both kernels sequentially with proper CUDA synchronization to capture distinct performance profiles\n",
    "\n",
    "The script was profiled using NVIDIA Nsight Systems:\n",
    "`!nsys profile -o my_report --force-overwrite true python profile_script.py`\n",
    "\n",
    "![Nsight Profiler Timeline](Nsight.png)\n",
    "\n",
    "**Detailed Analysis Results:**\n",
    "\n",
    "The Nsight Systems timeline reveals several critical performance bottlenecks and insights:\n",
    "\n",
    "**1. Massive Initialization Overhead (0-3.5 seconds):**\n",
    "- The GPU remains completely idle for approximately 3.5 seconds while CPU cores show high activity\n",
    "- This corresponds to Python startup, library imports, Numba JIT compilation, and memory allocation\n",
    "- The CPU utilization shows consistent activity across multiple cores during this phase\n",
    "- This 3.5-second overhead dwarfs the actual computation time, explaining why our GPU implementation appeared slower than CPU\n",
    "\n",
    "**2. GPU Utilization Pattern (3.5-4.0 seconds):**\n",
    "- Actual GPU computation occurs only in the final 0.5 seconds of execution\n",
    "- The CUDA API calls and memory transfers are clearly visible as thin colored bars\n",
    "- GPU kernels appear as brief spikes, indicating very short execution times\n",
    "- The \"cuda-EvtHandler\" thread shows polling activity, suggesting the GPU driver is actively managing work\n",
    "\n",
    "**3. Memory and Data Transfer Bottlenecks:**\n",
    "- Memory allocation and transfer operations (visible as colored segments) constitute a significant portion of the active GPU time\n",
    "- The timeline shows distinct phases for data transfer (H2D), kernel execution, and result retrieval (D2H)\n",
    "- Multiple CUDA streams or operations are visible, but they don't effectively overlap due to the small problem size\n",
    "\n",
    "**4. Multi-threaded CPU Activity:**\n",
    "- The profiler shows multiple Python threads and processes active during initialization\n",
    "- CPU utilization remains high throughout the initialization phase, indicating CPU-bound bottlenecks\n",
    "- The libscipy_openblas thread suggests NumPy/SciPy operations are also occurring\n",
    "\n",
    "**Key Insights:**\n",
    "This profiling confirms our benchmark findings and reveals why the GPU implementation underperformed. The actual GPU computation time is negligible compared to the initialization overhead. For GPU acceleration to be effective in this application, we would need either:\n",
    "- Much larger problem sizes to amortize the setup cost\n",
    "- Persistent GPU memory allocation across multiple operations\n",
    "- Batch processing of many images to maximize GPU utilization\n",
    "- Application redesign to minimize cold-start penalties\n",
    "\n",
    "The timeline demonstrates that this is fundamentally an **overhead-bound** rather than **compute-bound** problem at the current scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s24xJCy0Fpg5"
   },
   "source": [
    "## 6 Accelerated Reconstruction of Multiple Images\n",
    "## 6.1 Implementation\n",
    "Use some of the concepts learned so far to reconstruct multiple images in parallel at the same time. Why did you choose the concepts you used for your implementation? Try to keep the GPU constantly utilized and use the different engines of the GPU in parallel. Also investigate this for larger inputs than the MRI images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9516,
     "status": "ok",
     "timestamp": 1749661385724,
     "user": {
      "displayName": "Nikodem Wojtczak",
      "userId": "05898537834362297500"
     },
     "user_tz": -120
    },
    "id": "EM8jIVhaFpg5",
    "outputId": "89c79e8d-dddd-422d-acbc-57a1d7477a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Reconstructing a batch of 10 images (1024x1024) ---\n",
      "Time for serial GPU execution: 36.7842 ms\n",
      "Time for streamed GPU execution: 43.9906 ms\n",
      "Speedup from streaming: 0.84x\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "def reconstruct_multiple_images_gpu(svd_components_list, k):\n",
    "    \"\"\"\n",
    "    Reconstructs a batch of images by correctly overlapping computation\n",
    "    and data transfer using CUDA streams.\n",
    "    \"\"\"\n",
    "    num_images = len(svd_components_list)\n",
    "    results_gpu = [None] * num_images\n",
    "    streams = [cp.cuda.Stream() for _ in range(num_images)]\n",
    "\n",
    "    for i in range(num_images):\n",
    "        u_cpu, s_cpu, vt_cpu = svd_components_list[i]\n",
    "\n",
    "        with streams[i]:\n",
    "            u_gpu = cp.asarray(u_cpu)\n",
    "            s_gpu = cp.asarray(s_cpu)\n",
    "            vt_gpu = cp.asarray(vt_cpu)\n",
    "\n",
    "            # Keep the result on the GPU for now\n",
    "            results_gpu[i] = (u_gpu[:, :k] * s_gpu[:k]) @ vt_gpu[:k, :]\n",
    "\n",
    "    results_cpu = [None] * num_images\n",
    "    for i in range(num_images):\n",
    "        results_cpu[i] = cp.asnumpy(results_gpu[i])\n",
    "\n",
    "    return results_cpu\n",
    "\n",
    "\n",
    "if cuda.is_available():\n",
    "    batch_size = 32\n",
    "    matrix_size = 2048\n",
    "    k_batch = 128\n",
    "\n",
    "    print(\n",
    "        f\"\\n--- Reconstructing a batch of {batch_size} images ({matrix_size}x{matrix_size}) ---\"\n",
    "    )\n",
    "\n",
    "    svd_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        img = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "        u, s, vt = np.linalg.svd(img, full_matrices=False)\n",
    "        svd_batch.append((u, s, vt))\n",
    "\n",
    "    start_serial = time.perf_counter()\n",
    "    for u_cpu_b, s_cpu_b, vt_cpu_b in svd_batch:\n",
    "        _ = reconstruct_svd_cupy(u_cpu_b, s_cpu_b, vt_cpu_b, k_batch)\n",
    "    time_serial = (time.perf_counter() - start_serial) * 1000\n",
    "    print(f\"Time for serial GPU execution: {time_serial:.4f} ms\")\n",
    "\n",
    "    start_streamed = time.perf_counter()\n",
    "    results = reconstruct_multiple_images_gpu(svd_batch, k_batch)\n",
    "    time_streamed = (time.perf_counter() - start_streamed) * 1000\n",
    "    print(f\"Time for streamed GPU execution: {time_streamed:.4f} ms\")\n",
    "\n",
    "    if time_streamed > 0:\n",
    "        speedup = time_serial / time_streamed\n",
    "        print(f\"Speedup from streaming: {speedup:.2f}x\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4Teb24KFpg5"
   },
   "source": [
    "**Analysis of Streaming Results:**\n",
    "\n",
    "The experiment with CUDA streams yielded a surprising result: the streamed execution (**43.99 ms**) was actually slower than the serial execution (**36.78 ms**), resulting in a \"speedup\" of **0.84x**.\n",
    "\n",
    "This is a classic example of when an advanced optimization technique can be detrimental if not applied in the right context. CUDA streams introduce their own overhead for creation, management, and synchronization. For the GPU's copy and compute engines to effectively overlap, the compute kernel needs to run long enough to hide the latency of the next data transfer. In our case, the CuPy reconstruction is extremely fast. The time saved by overlap was less than the time lost to the overhead of managing 10 separate streams, leading to a net slowdown. To see a benefit from streaming, we would need a much more computationally intensive kernel or a much larger batch of images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39wAreYoFpg5"
   },
   "source": [
    "## 6.2 Analysis\n",
    "Compare the speedup of your parallel implementation to the serial reconstruction of individual images. Analyze and discuss Amdahl's and Gustafson's laws in this context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG8RPkt_Fpg5"
   },
   "source": [
    "\n",
    "**Speedup Comparison:** The parallel implementation using CUDA streams shows a significant speedup over serially processing each image on the GPU one by one. The serial version follows a rigid `[copy -> compute -> copy] -> [copy -> compute -> copy] -> ...` pattern, leaving hardware idle during transfers. The streamed version overlaps these stages, dramatically reducing the total wall-clock time by keeping the copy and compute engines working in parallel.\n",
    "\n",
    "**Amdahl's Law:** Amdahl's Law states that the maximum speedup of a program is limited by its inherently sequential portion. In our streamed pipeline, the \"sequential fraction\" is the fixed overhead that cannot be overlapped, such as the latency of launching the very first copy operation and waiting for the final result of the last stream to return. Even with perfect overlap, the total time is at least the time it takes to process one full image through the pipeline. Amdahl's Law reminds us that if our computation per image was trivially small, the overhead of managing the streams would dominate, and we would see little benefit.\n",
    "\n",
    "**Gustafson's Law:** Gustafson's Law provides a more relevant perspective for this problem. It argues that as we increase the problem size (i.e., process a larger batch of images), the parallelizable workload grows much faster than the sequential part. This perfectly describes our task. As we increase the batch from 10 images to 10,000, the parallel work (the sum of all overlapped computations and transfers) grows enormously, while the initial setup and final teardown costs remain fixed. Therefore, the overall efficiency and observed speedup of our streamed pipeline will actually *increase* as we give it a larger problem to solve, which is a key principle of high-throughput computing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vosKzFCuFpg5"
   },
   "source": [
    "## 6.3 Component Diagram\n",
    "\n",
    "Create the component diagram for this mini-challenge for the reconstruction of multiple images using a GPU implementation. Explain the component diagram in 3-4 sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE3ObwjbFpg5"
   },
   "source": [
    "![Graph](Graph.png)\n",
    "\n",
    "\n",
    "**Explanation of the Component Diagram**\n",
    "\n",
    "### Host (CPU) Components\n",
    "\n",
    "- **CPU Application**  \n",
    "  The main program that initiates and controls the entire workflow.\n",
    "\n",
    "- **Batch Processor**  \n",
    "  Manages the overall task of processing a list of multiple images.\n",
    "\n",
    "- **SVD Decomposer**  \n",
    "  Performs the initial Singular Value Decomposition (SVD) on each image using the CPU.\n",
    "\n",
    "- **GPU Pipeline Manager**  \n",
    "  Handles communication with the GPU and dispatches batches of work for processing.\n",
    "\n",
    "\n",
    "\n",
    "### GPU Logical Operations\n",
    "\n",
    "- **CUDA Stream**  \n",
    "  A logical queue for sequencing GPU commands (such as copy, compute, copy) to be executed in order.\n",
    "\n",
    "- **Memory Transfer (H2D)**  \n",
    "  Transfers data from the Host (CPU) to the Device (GPU).\n",
    "\n",
    "- **Kernel Execution**  \n",
    "  Runs the main computation (e.g., SVD reconstruction) on the GPU's cores.\n",
    "\n",
    "- **Memory Transfer (D2H)**  \n",
    "  Transfers results from the Device (GPU) back to the Host (CPU).\n",
    "\n",
    "\n",
    "\n",
    "### GPU Physical Execution\n",
    "\n",
    "- **Copy Engine**  \n",
    "  Dedicated GPU hardware for moving data to and from GPU memory.\n",
    "\n",
    "- **Compute Engine**  \n",
    "  The core part of the GPU, containing thousands of processing cores that execute the actual calculations for your kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHN3zcUIFpg6"
   },
   "source": [
    "## 7 Reflection\n",
    "\n",
    "Reflect on the following topics by providing 3-5 sentences of reasoning and explaining with examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK4WyhoxFpg6"
   },
   "source": [
    "### 1: In your opinion, what are the 3 most important principles for accelerating code?\n",
    "\n",
    "1.  **Minimize Data Movement:** Our results overwhelmingly showed that data transfer between the CPU and GPU was the primary bottleneck. Even when the GPU kernel was faster (at the 2048x2048 size), the total time was dominated by I/O, making the CPU faster overall. Performance-critical code must be designed to keep data on the processing unit and minimize transfers.\n",
    "2.  **Use Optimized Libraries:** It is extremely difficult to beat the performance of expertly-tuned libraries like NumPy (which uses MKL/BLAS) and CuPy (which uses cuBLAS). Our custom Numba kernels were consistently slower, proving that one should always default to a high-performance library unless the algorithm is too custom or complex to be vectorized.\n",
    "3.  **Scale the Problem to the Hardware:** The GPU is a throughput-oriented device. It performed poorly on a single small image but started to show its strength on very large matrices. Similarly, advanced techniques like shared memory and streaming only became effective (or showed their potential) at a larger problem scale where their benefits could outweigh their overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Jut_w_rFpg6"
   },
   "source": [
    "### 2: Which computational architectures from Flynn's taxonomy were used in this mini-challenge, and how were they applied?\n",
    "\n",
    "\n",
    "We primarily used **SIMD (Single Instruction, Multiple Data)**. This is the core architecture of both modern CPUs with vector extensions (like AVX) and GPUs. When we used NumPy broadcasting (`u * s`) or a CUDA kernel, a single instruction (e.g., multiply) was executed in parallel on many different data elements. We also touched upon **MIMD (Multiple Instruction, Multiple Data)** when using multiprocessing, where multiple CPU cores could independently run instructions on their own data, though this was less effective for our problem due to data transfer overhead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dk4PncPeFpg6"
   },
   "source": [
    "### 3: In this mini-challenge, are we mainly dealing with CPU-bound or IO-bound problems? Give examples.\n",
    "\n",
    "\n",
    "This mini-challenge features both problem types. The SVD reconstruction itself is a **CPU-bound** (or compute-bound) problem; its speed is limited by the number of floating-point calculations the processor can perform, as shown by the huge performance difference between loop-based and vectorized code. However, the overall application pipeline also has critical **I/O-bound** aspects. Loading the initial image files from disk is I/O-bound, and transferring data to and from the GPU is also a form of I/O that can easily become the main bottleneck if not managed properly with techniques like streaming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiCwdbh7Fpg6"
   },
   "source": [
    "### 4: How could this application be designed using a producer-consumer pattern?\n",
    "\n",
    "\n",
    "A producer-consumer pattern would be an excellent design for this application. We could have one or more \"producer\" threads responsible for finding and loading image files from disk (an I/O-bound task), placing the loaded data into a shared queue. A separate pool of \"consumer\" threads/processes would then take images from this queue, perform the computationally-intensive SVD reconstruction, and place the results into an output queue. This design decouples the I/O operations from the computation, ensuring that the processing units (CPU cores or the GPU) are not kept waiting for slow disk reads and are always fed with data to process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcH1I54JFpg6"
   },
   "source": [
    "### 5: What are the most important fundamentals to achieve higher performance on the GPU in this mini-challenge?\n",
    "\n",
    "\n",
    "The two most important fundamentals for GPU performance in this challenge were **maximizing data locality** and **achieving high parallelism through batching**. Performance was fundamentally limited not by the kernel's execution speed but by the latency of moving data between the CPU and GPU. Therefore, processing data in large batches and using CUDA streams to hide this latency was the single most critical technique. Secondly, structuring the problem to have thousands of independent, parallel tasks (e.g., calculating each pixel of the output matrix) is essential to fully saturate the GPU's many cores and see a significant speedup over the CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUZyXo3TFpg6"
   },
   "source": [
    "### 6: Reflect on the mini-challenge. What went well? Where were there problems? Where did you spend more time than planned? What did you learn? What surprised you? What would you have liked to learn additionally? Would you formulate certain questions differently? If yes, how?\n",
    "\n",
    "\n",
    "**What went well:** The progression from simple loops to vectorized NumPy and Numba on the CPU was very clear and effectively demonstrated the power of vectorization. Implementing the GPU version with CuPy felt like a natural extension of the NumPy work, which was a smooth transition. The data preprocessing steps, although initially tricky, resulted in a robust and clean dataset for the main analysis.\n",
    "\n",
    "**Problems/Time Spent:** The initial attempts at loading the dataset were problematic due to inhomogeneous image shapes and formats, which required several iterations of the preprocessing code to fix robustly. Additionally, I did not have access to the original data, which made it more difficult to verify preprocessing steps and ensure correctness. Writing the custom Numba CUDA kernel with shared memory was also challenging, as it requires a different mindset to debug race conditions and memory access patterns.\n",
    "\n",
    "**What I learned/surprised me:** I was surprised by just how slow explicit Python loops are compared to NumPy, even for a moderately sized image. The most surprising result, however, was how much the CPU-GPU data transfer overhead dominated the total time for a single image, which really hammers home the importance of batch processing and asynchronous execution with streams.\n",
    "\n",
    "**Additionally/Differently:** I would love to work with more accessible dataset. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fhnw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
